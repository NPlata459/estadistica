# =============================================================
# ANÁLISIS AUTOMÁTICO: PUNTOS 1 A 9 (bloque único)
# Diseñado para Colab o entorno local. Funciona con o sin el Excel.
# =============================================================

# --- Importaciones ---
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import statsmodels.api as sm
from statsmodels.stats.anova import anova_lm
from statsmodels.stats.outliers_influence import summary_table
import warnings
from math import sqrt
from IPython.display import display, Markdown

warnings.filterwarnings("ignore")
plt.rcParams.update({"figure.max_open_warning": 0})

# --- Parámetros ---
alpha = 0.05
excel_file = "Tablas_Regresion_Lineal.xlsx"

# =============================================================
# 1) CARGA DEL EXCEL (si existe) O CREACIÓN DE LOS DATASETS DEL PDF
# =============================================================
if os.path.exists(excel_file):
    print("✅ Archivo Excel encontrado. Leyendo hojas...")
    xls = pd.ExcelFile(excel_file)
    # cargar hojas en lista (se asume que de izquierda a derecha son puntos 1..9)
    sheets = {}
    for i, sheet_name in enumerate(xls.sheet_names[:9], start=1):
        if i == 4: # Special handling for Punto 4 (multiple regions in one sheet)
            df_raw = pd.read_excel(xls, sheet_name=sheet_name, header=None) # Read without header initially

            # Find the starting columns for each region based on markers in the second row (index 1)
            # This is a heuristic and might need adjustment if Excel format changes
            r1_start_col = df_raw.iloc[1].loc[df_raw.iloc[1] == 'REGION 1'].index[0]
            r2_start_col = df_raw.iloc[1].loc[df_raw.iloc[1] == 'REGION 2'].index[0]
            r3_start_col = df_raw.iloc[1].loc[df_raw.iloc[1] == 'REGION 3'].index[0]

            # Extract data for each region, starting from row 4 (index 3) after headers, and drop NaNs
            r1_data = df_raw.iloc[3:].loc[:, [r1_start_col, r1_start_col + 1]].dropna()
            r2_data = df_raw.iloc[3:].loc[:, [r2_start_col, r2_start_col + 1]].dropna()
            r3_data = df_raw.iloc[3:].loc[:, [r3_start_col, r3_start_col + 1]].dropna()

            # Assign generic column names 'X' and 'Y' and convert to float
            r1 = pd.DataFrame(r1_data.values, columns=['X', 'Y']).astype(float)
            r2 = pd.DataFrame(r2_data.values, columns=['X', 'Y']).astype(float)
            r3 = pd.DataFrame(r3_data.values, columns=['X', 'Y']).astype(float)

            sheets[4] = {"R1": r1, "R2": r2, "R3": r3}
            print(f"  - Punto 4: hoja '{sheet_name}' cargada y dividida en 3 regiones.")
        else:
            sheets[i] = pd.read_excel(xls, sheet_name=sheet_name)
            print(f"  - Punto {i}: hoja '{sheet_name}' cargada. Dimensiones: {sheets[i].shape}")
else:
    print("⚠️ Excel no encontrado. Creando datasets desde las tablas del PDF (valores fijos).")
    sheets = {}

    # Punto 1: Goles anotados (X) - Goles recibidos (Y)
    sheets[1] = pd.DataFrame({
        "Equipo": ["Leones","Águilas","Tigres","Toros","Pumas","Jaguares"],
        "X": [38,45,31,50,27,41],
        "Y": [22,28,39,33,36,29]
    })

    # Punto 2: Horas estudio (X) y calificación final (Y) - 12 observaciones
    sheets[2] = pd.DataFrame({
        "X": [4,7,10,6,8,9,5,11,7,6,8,10],
        "Y": [65,70,85,78,82,91,74,94,80,77,84,88]
    })

    # Punto 3: Llamadas diarias (X) Ventas semanales (Y) - 10 obs
    sheets[3] = pd.DataFrame({
        "X": [15,20,18,25,22,17,19,23,21,16],
        "Y": [5,7,6,11,9,6,7,10,8,5]
    })

    # Punto 4: Three regions (cada región como df)
    # Region 1 (X tamaño, Y ventas)
    r1 = pd.DataFrame({
        "X":[3.7,5.0,7.0,6.5,2.2,5.5,2.9],
        "Y":[9.18,8.54,6.85,8.98,5.20,7.15,7.88]
    })
    # Region 2
    r2 = pd.DataFrame({
        "X":[4.2,3.4,2.5,1.5,3.7,4.9,3.2],
        "Y":[5.65,3.26,2.25,1.95,3.34,7.23,4.45]
    })
    # Region 3
    r3 = pd.DataFrame({
        "X":[2.2,1.6,2.3,2.9,3.2,3.7,4.4,4.8,5.0],
        "Y":[2.05,1.43,2.44,2.95,3.16,3.95,4.25,5.39,5.64]
    })
    # Combine into a single sheet-like structure (we will analyze cada región por separado)
    sheets[4] = {"R1": r1, "R2": r2, "R3": r3}

    # Punto 5: Tiempo vs Población (crecimiento acelerado)
    sheets[5] = pd.DataFrame({
        "X":[1,2,3,4,5,6,7],
        "Y":[45,112,228,485,900,1720,3526]
    })

    # Punto 6: Ingreso (X, miles) y Gasto en alimentos (Y, cientos)
    sheets[6] = pd.DataFrame({
        "X":[1.2,1.8,2.4,3.5,5.0,7.5,10.0],
        "Y":[2.1,2.7,3.0,4.2,5.0,6.5,7.3]
    })

    # Punto 7: Intensidad (X) Concentración (Y)
    sheets[7] = pd.DataFrame({
        "X":[10,30,50,70,90,110,130],
        "Y":[2.2,3.8,6.1,11.7,19.2,24.2,36.9]
    })

    # Punto 8: Crecimiento bacterias (similar a 5, en Cali)
    sheets[8] = pd.DataFrame({
        "X":[1,2,3,4,5,6,7],
        "Y":[42,110,220,495,910,1730,3500]
    })

    # Punto 9: Distancia (mi) y Tarifa (USD) - 20 observaciones
    sheets[9] = pd.DataFrame({
        "Distancia":[636,275,398,405,286,627,2346,177,2528,248,512,248,248,237,621,853,2181,1531,2724,999],
        "Tarifa":[109,129,141,152,165,259,231,148,224,125,225,124,125,137,191,191,249,229,243,219]
    })

print("\n--- Datasets listos. Iniciando análisis punto por punto ---\n")

# =============================================================
# FUNCIONES AUXILIARES
# =============================================================
def shapiro_test(series, alpha=alpha):
    stat, p = stats.shapiro(series)
    return stat, p, ("normal" if p>alpha else "no normal")

def correlation_tests(x, y):
    pear_r, pear_p = stats.pearsonr(x, y)
    spear_r, spear_p = stats.spearmanr(x, y)
    kend_r, kend_p = stats.kendalltau(x, y)
    return {
        "pearson": (pear_r, pear_p),
        "spearman": (spear_r, spear_p),
        "kendall": (kend_r, kend_p)
    }

def t_for_correlation(r, n):
    # t = r * sqrt((n-2)/(1-r^2))
    if abs(r)==1:
        return np.inf
    t_stat = r * np.sqrt((n-2)/(1 - r**2))
    df = n - 2
    p = 2*(1 - stats.t.cdf(abs(t_stat), df))
    return t_stat, df, p

def regression_ols(x, y, add_const=True):
    X = sm.add_constant(x) if add_const else x
    model = sm.OLS(y, X).fit()
    return model

def compute_conf_pred_intervals(model, x, alpha=alpha):
    # x: array-like single x value(s) OR 1D array of predictor values (without constant).
    # Will return prediction and confidence intervals for each x in array-like.
    st, data, ss2 = summary_table(model, alpha=alpha)
    fitted = data[:,2]
    predict_mean_se = data[:,3]
    predict_mean_ci_low = data[:,4]
    predict_mean_ci_upp = data[:,5]
    predict_obs_ci_low = data[:,6]
    predict_obs_ci_upp = data[:,7]
    return {
        "fitted": fitted,
        "mean_ci_low": predict_mean_ci_low,
        "mean_ci_upp": predict_mean_ci_upp,
        "obs_ci_low": predict_obs_ci_low,
        "obs_ci_upp": predict_obs_ci_upp
    }

def anova_manual(y, y_hat, p=1):
    n = len(y)
    y_bar = np.mean(y)
    SST = np.sum((y - y_bar)**2)
    SSE = np.sum((y - y_hat)**2)
    SSR = SST - SSE
    df_reg = p
    df_err = n - p - 1
    MSR = SSR/df_reg
    MSE = SSE/df_err
    F = MSR/MSE
    p_val = 1 - stats.f.cdf(F, df_reg, df_err)
    return {"SST":SST,"SSE":SSE,"SSR":SSR,"MSR":MSR,"MSE":MSE,"F":F,"p":p_val,"df_reg":df_reg,"df_err":df_err}

def plot_residual_diagnostics(model, x, y, title_prefix=""):
    resid = model.resid
    fitted = model.fittedvalues
    fig = plt.figure(figsize=(14,10))
    # Residuales vs Fitted
    ax1 = fig.add_subplot(221)
    ax1.scatter(fitted, resid)
    ax1.axhline(0, linestyle='--', color='red')
    ax1.set_xlabel("Valores ajustados")
    ax1.set_ylabel("Residuales")
    ax1.set_title(f"{title_prefix} Residuales vs Valores ajustados")
    # Residuales vs X
    ax2 = fig.add_subplot(222)
    ax2.scatter(x, resid)
    ax2.axhline(0, linestyle='--', color='red')
    ax2.set_xlabel("X")
    ax2.set_ylabel("Residuales")
    ax2.set_title(f"{title_prefix} Residuales vs X")
    # Histograma
    ax3 = fig.add_subplot(223)
    ax3.hist(resid, bins=10, edgecolor='k')
    ax3.set_title(f"{title_prefix} Histograma residuales")
    # QQ-plot
    ax4 = fig.add_subplot(224)
    sm.qqplot(resid, line='45', ax=ax4)
    ax4.set_title(f"{title_prefix} Q-Q plot residuales")
    plt.tight_layout()
    plt.show()
    # Shapiro
    sw_stat, sw_p, sw_res = shapiro_test(resid)
    print(f"Shapiro-Wilk residuales: stat={sw_stat:.4f}, p={sw_p:.4f} -> {sw_res}")
    # Outliers estandarizados
    syx = np.sqrt(np.sum(resid**2)/(len(resid)-2))
    estd = resid / syx
    n_outliers = np.sum(np.abs(estd) > 2.5)
    print(f"Desviación estándar de estimación sy,x = {syx:.4f} | Outliers (|estd|>2.5): {n_outliers}")
    return resid, estd

def print_model_summary(model):
    print(model.summary())

# =============================================================
# 2) FUNCION GENERAL: EJECUTAR PROTOCOLO DE REGRESIÓN (SIMPLE)
# =============================================================
def ejecutar_protocolo(df, x_col, y_col, punto_label="Punto", transform=None):
    """
    transform: None, "log_y", "log_x_y", "sqrt_y", "log_log", etc.
    """
    display(Markdown(f"## {punto_label} — Variables: X='{x_col}' Y='{y_col}'"))
    X = df[x_col].astype(float)
    Y = df[y_col].astype(float)

    # 1. EDA
    display(Markdown("### 1) Análisis exploratorio inicial"))
    display(pd.concat([X.describe().rename("X"), Y.describe().rename("Y")], axis=1))
    # Histogramas
    fig, axs = plt.subplots(1,2, figsize=(12,4))
    axs[0].hist(X, bins=10, edgecolor='k'); axs[0].set_title(f"Histograma X ({x_col})")
    axs[1].hist(Y, bins=10, edgecolor='k'); axs[1].set_title(f"Histograma Y ({y_col})")
    plt.show()
    # Scatter
    plt.figure(figsize=(6,4))
    plt.scatter(X, Y)
    plt.xlabel(x_col); plt.ylabel(y_col)
    plt.title(f"Diagrama de dispersión: {x_col} vs {y_col}")
    plt.show()

    # 2. Normalidad (Shapiro)
    display(Markdown("### 2) Prueba de Normalidad (Shapiro-Wilk)"))
    sx_stat, sx_p, sx_res = shapiro_test(X)
    sy_stat, sy_p, sy_res = shapiro_test(Y)
    print(f"X: stat={sx_stat:.4f}, p={sx_p:.4f} -> {sx_res}")
    print(f"Y: stat={sy_stat:.4f}, p={sy_p:.4f} -> {sy_res}")

    # 3. Correlaciones
    display(Markdown("### 3) Análisis de correlación (Pearson, Spearman, Kendall)"))
    cors = correlation_tests(X, Y)
    for k,v in cors.items():
        print(f"{k.title()}: r={v[0]:.4f}, p={v[1]:.4f}")

    # Select coefficient guidance
    use = "Pearson" if (sx_p>alpha and sy_p>alpha) else "Spearman/Kendall"
    print(f"Selección recomendada de coeficiente: {use}")

    # 4. Prueba de hipótesis para correlación (t)
    display(Markdown("### 4) Prueba de hipótesis para la correlación (dos colas)"))
    n = len(X)
    r = cors["pearson"][0]
    t_stat, df_t, p_corr = t_for_correlation(r, n)
    t_crit = stats.t.ppf(1-alpha/2, df_t)
    print(f"n={n}, r(pearson)={r:.4f}")
    print(f"t estadístico = {t_stat:.4f}, gl={df_t}, t_crit (α/2) = ±{t_crit:.4f}, p = {p_corr:.4f}")
    print("Decisión: " + ("Rechazar H0 (correlación significativa)" if p_corr<alpha else "No rechazar H0 (no significativa)"))

    # 5. Modelo de regresión lineal - si requiere transformación, aplicarla
    display(Markdown("### 5) Ajuste del modelo de regresión lineal"))
    X_used = X.copy()
    Y_used = Y.copy()
    if transform is not None:
        display(Markdown(f"Aplicando transformación: {transform}"))
        if transform == "log_y":
            Y_used = np.log(Y_used)
        elif transform == "log_x_y":
            X_used = np.log(X_used)
            Y_used = np.log(Y_used)
        elif transform == "sqrt_y":
            Y_used = np.sqrt(Y_used)
        elif transform == "log_log":
            X_used = np.log(X_used)
            Y_used = np.log(Y_used)
        else:
            print("Transformación no reconocida. No se aplica.")
    # Ajuste
    model = regression_ols(X_used, Y_used)
    print_model_summary(model)

    # Compute slope/intercept via formulas as double-check
    x_bar = X_used.mean()
    y_bar = Y_used.mean()
    r_used = np.corrcoef(X_used, Y_used)[0,1]
    s_x = X_used.std(ddof=0)
    s_y = Y_used.std(ddof=0)
    b1_formula = r_used * (s_y / s_x)
    b0_formula = y_bar - b1_formula * x_bar
    print(f" (Verif) b1 por fórmula = {b1_formula:.6f}, b0 = {b0_formula:.6f}")

    # 6. Prueba de significancia de la pendiente
    display(Markdown("### 6) Prueba de significancia de la pendiente"))
    b1 = model.params[1]
    se_b1 = model.bse[1]
    t_b1 = b1 / se_b1
    p_b1 = model.pvalues[1]
    df_b1 = model.df_resid
    t_crit_b1 = stats.t.ppf(1-alpha/2, df_b1)
    print(f"b1 = {b1:.6f}, sb = {se_b1:.6f}, t = {t_b1:.4f}, gl = {int(df_b1)}, t_crit = ±{t_crit_b1:.4f}, p = {p_b1:.4f}")
    print("Decisión: " + ("Pendiente significativa (rechazar H0)" if p_b1<alpha else "Pendiente no significativa"))

    # Predicciones y errores
    y_hat = model.fittedvalues
    resid = model.resid

    # 7. Coeficiente de determinación R^2
    display(Markdown("### 7) Coeficiente de determinación (R^2) y particiones"))
    R2 = model.rsquared
    print(f"R^2 = {R2:.4f} -> {R2*100:.2f}% de la variabilidad de Y explicada por X")
    # ANOVA tabla (manual)
    an = anova_manual(Y_used.values, y_hat.values, p=1)
    print("ANOVA (manual):")
    for k,v in an.items():
        print(f"  {k}: {v}")

    # 8. Intervalos de confianza y predicción (mostrar para cada punto de X)
    display(Markdown("### 8) Intervalos de confianza y predicción (para los datos)"))
    sti = compute_conf_pred_intervals(model, X_used)
    # Graficar fit con IC de la media y IP de predicción
    plt.figure(figsize=(8,5))
    plt.scatter(X_used, Y_used, label="Datos")
    # ordenar por X para trazar líneas
    order = np.argsort(X_used)
    xs = np.array(X_used)[order]
    plt.plot(xs, sti["fitted"][order], label="Ajuste", color='red')
    plt.fill_between(xs, sti["mean_ci_low"][order], sti["mean_ci_upp"][order], alpha=0.2, label="IC media")
    plt.fill_between(xs, sti["obs_ci_low"][order], sti["obs_ci_upp"][order], alpha=0.1, label="IP observación")
    plt.xlabel(x_col); plt.ylabel(y_col)
    plt.title(f"{punto_label} - Ajuste y CIs")
    plt.legend()
    plt.show()

    # 9. Análisis de residuos (gráficos, Shapiro, outliers)
    display(Markdown("### 9) Análisis de residuos y diagnóstico"))
    resid, estd = plot_residual_diagnostics(model, X_used, Y_used, title_prefix=punto_label)
    # Recomendaciones rápidas
    print("\nConclusión de supuestos (breve):")
    print("- Linealidad: revisar Residuales vs X y Residuales vs Predichos.")
    print("- Normalidad: ver Shapiro-Wilk de residuales.")
    print("- Homocedasticidad: ver patrón en Residuales vs Predichos.")
    print("- Independencia: depende del diseño de muestreo (no analizado aquí).")

    display(Markdown("--- FIN del punto ---"))
    print("\n\n")

# =============================================================
# 3) EJECUTAR PUNTO A PUNTO (1..9)
# =============================================================

# Punto 1
if 1 in sheets:
    df1 = sheets[1].copy()
    # si existe columna 'Equipo', renombrar X/Y si es necesario
    if "X" not in df1.columns:
        # intentar detectar nombres
        cols = df1.columns.tolist()
        # Assume the first non-Equipo column is X and the second is Y
        x_col_df1 = cols[1] if 'Equipo' in cols else cols[0]
        y_col_df1 = cols[2] if 'Equipo' in cols else cols[1]
        df1 = df1.rename(columns={x_col_df1:"X", y_col_df1:"Y"})
    ejecutar_protocolo(df1, "X", "Y", punto_label="PUNTO 1 - Goles (X) vs Goles recibidos (Y)")

# Punto 2
if 2 in sheets:
    df2 = sheets[2].copy()
    # Renombrar columnas para consistencia con la función ejecutar_protocolo
    if "X" not in df2.columns and "Y" not in df2.columns:
        cols = df2.columns.tolist()
        # Assuming the first column is X and the second is Y
        df2 = df2.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df2, "X", "Y", punto_label="PUNTO 2 - Horas estudio (X) vs Calificación (Y)")

# Punto 3
if 3 in sheets:
    df3 = sheets[3].copy()
    # Renombrar columnas para consistencia con la función ejecutar_protocolo
    if "X" not in df3.columns and "Y" not in df3.columns:
        cols = df3.columns.tolist()
        df3 = df3.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df3, "X", "Y", punto_label="PUNTO 3 - Llamadas diarias (X) vs Ventas semanales (Y)")

# Punto 4 (tres regiones)
if 4 in sheets:
    regions = sheets[4]
    for key, rdf in regions.items():
        # The DataFrames r1, r2, r3 are already constructed with 'X' and 'Y' columns.
        # The explicit renaming check is no longer needed here.
        ejecutar_protocolo(rdf, "X", "Y", punto_label=f"PUNTO 4 - {key} Tamaño tienda (X) vs Ventas (Y)")
    # adicional: análisis combinado (opcional)
    combined = pd.concat([regions["R1"].assign(Region='R1'),
                          regions["R2"].assign(Region='R2'),
                          regions["R3"].assign(Region='R3')], ignore_index=True)
    # Asegurar que las columnas 'X' y 'Y' existen en el combinado para el análisis opcional
    if "X" not in combined.columns and "Y" not in combined.columns:
        # Esto es un manejo de caso para si los DataFrames individuales de 'regions' no fueron renombrados antes de concat
        # En este escenario, es más complejo si las columnas originales no son consistentes.
        # Asumimos que los df de regiones YA tienen X e Y si se ejecutan individualmente.
        pass # Si ya se renombraron en el bucle anterior, 'combined' ya debería tener 'X' y 'Y'
    ejecutar_protocolo(combined, "X", "Y", punto_label="PUNTO 4 - COMBINADO (todas regiones)")

# Punto 5 - transformation comparison (original vs log Y)
if 5 in sheets:
    df5 = sheets[5].copy()
    # Renombrar columnas para consistencia
    if "X" not in df5.columns and "Y" not in df5.columns:
        cols = df5.columns.tolist()
        df5 = df5.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df5, "X", "Y", punto_label="PUNTO 5 - Tiempo vs Población (modelo original)")
    # Transformación log Y
    ejecutar_protocolo(df5, "X", "Y", punto_label="PUNTO 5 - Transformación log(Y)", transform="log_y")

# Punto 6 - original vs log-log
if 6 in sheets:
    df6 = sheets[6].copy()
    # Renombrar columnas para consistencia
    if "X" not in df6.columns and "Y" not in df6.columns:
        cols = df6.columns.tolist()
        df6 = df6.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df6, "X", "Y", punto_label="PUNTO 6 - Ingreso vs Gasto (modelo original)")
    ejecutar_protocolo(df6, "X", "Y", punto_label="PUNTO 6 - Transformación log-log (log X, log Y)", transform="log_log")

# Punto 7 - original vs sqrt(Y)
if 7 in sheets:
    df7 = sheets[7].copy()
    # Renombrar columnas para consistencia
    if "X" not in df7.columns and "Y" not in df7.columns:
        cols = df7.columns.tolist()
        df7 = df7.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df7, "X", "Y", punto_label="PUNTO 7 - Intensidad vs Concentración (original)")
    ejecutar_protocolo(df7, "X", "Y", punto_label="PUNTO 7 - Transformación sqrt(Y)", transform="sqrt_y")

# Punto 8 - similar a punto 5 (log Y)
if 8 in sheets:
    df8 = sheets[8].copy()
    # Renombrar columnas para consistencia
    if "X" not in df8.columns and "Y" not in df8.columns:
        cols = df8.columns.tolist()
        df8 = df8.rename(columns={cols[0]:"X", cols[1]:"Y"})
    ejecutar_protocolo(df8, "X", "Y", punto_label="PUNTO 8 - Tiempo vs Población (original)")
    ejecutar_protocolo(df8, "X", "Y", punto_label="PUNTO 8 - Transformación log(Y)", transform="log_y")

# Punto 9 - Distancia vs Tarifa
if 9 in sheets:
    df9 = sheets[9].copy()
    # Explicitly select and rename the correct numerical columns
    # This assumes 'Distancia (mi)' and 'Tarifa (USD)' are always the numerical columns to use.
    if 'Distancia (mi)' in df9.columns and 'Tarifa (USD)' in df9.columns:
        df9 = df9[['Distancia (mi)', 'Tarifa (USD)']].rename(columns={"Distancia (mi)":"X","Tarifa (USD)":"Y"})
    else:
        print("Error: Columnas 'Distancia (mi)' o 'Tarifa (USD)' no encontradas en el Punto 9.")
        # Handle the error appropriately, e.g., skip this point or raise an exception
        # For now, we will assume these columns are present.

    ejecutar_protocolo(df9, "X", "Y", punto_label="PUNTO 9 - Distancia (X) vs Tarifa (Y)")
    # Estimación tarifa para 1500 millas y advertencia extrapolación para 4218
    # Asegurarse de que X y Y están correctamente establecidos en df9 para el modelo
    model9 = regression_ols(df9["X"], df9["Y"])
    b0 = model9.params[0]; b1 = model9.params[1]
    est_1500 = b0 + b1*1500
    print(f"Estimación tarifa para 1500 millas (usando modelo): ${est_1500:.2f}")
    print("Advertencia: No extrapolar lejos del rango observado; ejemplo: 4218 millas está fuera del rango, la predicción puede no ser razonable.")

print("\n=== FIN DEL SCRIPT: análisis completos para Puntos 1 a 9 ===")
